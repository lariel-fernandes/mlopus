{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "016bd3e1-18fd-44fd-9c87-c4efa3070868",
      "metadata": {},
      "source": [
        "### Ad hoc evaluation\n",
        "This notebook is an example/template for ad hoc explorations/evaluations tracked on MLflow.\n",
        "It can be used in the early stages of the development of a ML task, when designing a production-ready Kedro pipeline might still be a bit of an overkill/over-engineering.\n",
        "\n",
        "Run from the `poetry shell`, in the root dir of this example project:\n",
        "```bash\n",
        "NB=adhoc_evaluation.ipynb\n",
        "\n",
        "papermill notebooks/$NB logs/$NB --cwd notebooks \\\n",
        "        -p tags '{}' \\\n",
        "        -p params '{}' \\\n",
        "        -p tracking '{}'\n",
        "```\n",
        "Contents for `tags`, `params` and `tracking` are JSON-encoded.\n",
        "\n",
        "An interrupted run can be resumed by adding `\"run_id\": \"...\"` to the `tracking` argument.\n",
        "\n",
        "See more options with `papermill --help`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba374e9b-4e43-4ca8-a89b-4da7f5b4746c",
      "metadata": {},
      "outputs": [],
      "source": [
        "%env KEDRO_LOGGING_CONFIG ../conf/logging.yml\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import functools\n",
        "import contextlib\n",
        "from devtools import pprint\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "import mlopus\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from kedro.framework.startup import bootstrap_project\n",
        "from kedro.framework.session import KedroSession\n",
        "from mlopus.utils import dicts, iter_utils\n",
        "from pydantic.v1 import BaseModel\n",
        "\n",
        "from mlopus_kedro_example.nodes import EvalModel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0840354-6270-4533-9d5a-ff6f2cce3871",
      "metadata": {},
      "source": [
        "### Reuse the default Kedro session\n",
        "Useful for inferring some project attributes (e.g.: Git details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d567d7-416f-47fc-a5f5-d0e690591594",
      "metadata": {},
      "outputs": [],
      "source": [
        "bootstrap_project(\"../\")\n",
        "session = KedroSession.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32503940-1e6f-420a-bde4-20f0cb12d60f",
      "metadata": {},
      "source": [
        "### MLflow tags\n",
        "Evaluated by combining the defaults with the command-line argument `-p tags '{}'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0968c62-8943-4b82-89ae-f5387ae0e83b",
      "metadata": {},
      "outputs": [],
      "source": [
        "tags = dicts.deep_merge(\n",
        "    {\n",
        "        \"mlflow\": {\n",
        "            \"user\": session.store[\"username\"],\n",
        "            \"source\": {\"commit\": session.store[\"git\"][\"commit_sha\"]},\n",
        "        },\n",
        "    },\n",
        "    json.loads(x) if isinstance(x := locals().get(\"tags\", {}), str) else x,\n",
        ")\n",
        "\n",
        "pprint(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44d7f71-611d-4e9d-b936-568d9667366a",
      "metadata": {},
      "source": [
        "### MLflow params\n",
        "Evaluated by combining the defaults with the command-line argument `-p params '{}'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30cc915b-db8c-436a-be28-f7be2a4e8b29",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Params(BaseModel):\n",
        "    k_values: list[int] = [3, 10, 20, 50]\n",
        "    model_versions: set[str] = {\"1\", \"2\"}\n",
        "    model_name: str = \"mlopus_kedro_example\"\n",
        "\n",
        "pprint(params := Params.parse_obj(json.loads(x) if isinstance(x := locals().get(\"params\", {}), str) else x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e04d11-d9ab-4aec-afbe-3227b5b90879",
      "metadata": {},
      "source": [
        "### Experiment tracking settings\n",
        "Evaluated by combining the defaults with the command-line argument `-p tracking '{}'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1aa52e4-3006-43e4-9dfe-dca438441446",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tracking(BaseModel):\n",
        "    mlflow: dict = {}\n",
        "    run_id: str | None = None\n",
        "    run_name: str | None = None\n",
        "    exp_name: str = \"mlopus_kedro_example\"\n",
        "\n",
        "pprint(tracking := Tracking.parse_obj(json.loads(x) if isinstance(x := locals().get(\"tracking\", {}), str) else x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afc73b83-bbda-4370-b716-201162502563",
      "metadata": {},
      "source": [
        "### Experiment logic\n",
        "Yield metrics and artifacts for each evaluated model version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6483dfeb-f512-49f7-a09d-22b5454d9a70",
      "metadata": {},
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def experiment_logic(model_version: str) -> Tuple[dict, dict]:\n",
        "        \n",
        "    # Load model using the `default` schema and evaluate to get metrics\n",
        "    metrics = EvalModel(k_values=params.k_values)(\n",
        "        model := mlopus.artschema.load_artifact(\n",
        "            schema=\"default\",\n",
        "            subject=mlflow.get_model(params.model_name).get_version(model_version),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Save some temporary artifacts\n",
        "    with TemporaryDirectory() as tmp:\n",
        "        np.save(\n",
        "            file=(vectors := Path(tmp) / \"vectors.npy\"),\n",
        "            arr=model.get_vectors_by_labels(model.labels),\n",
        "        )\n",
        "\n",
        "        # Define a mapping of `file_name` to `file_path_or_dumper`\n",
        "        artifacts = {\n",
        "            vectors.name: vectors,\n",
        "            \"metrics.json\": lambda path: path.write_text(json.dumps(metrics)),\n",
        "        }\n",
        "    \n",
        "        yield metrics, artifacts  # Yield metrics and artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf2fd18-ae08-4805-97ec-8516c94fcddc",
      "metadata": {},
      "source": [
        "### Experiment loop\n",
        "Run the experiment logic with different model versions while recording tags, params, metrics and artifacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a480a1-79d8-4266-ad10-6a1977c1a2f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "mlflow = mlopus.mlflow.get_api(**tracking.mlflow)  # Get MLflow API\n",
        "\n",
        "with (  # Resume parent run by ID or start a new one\n",
        "    mlflow.resume_run(tracking.run_id) if tracking.run_id else\n",
        "    mlflow.get_or_create_exp(tracking.exp_name).start_run(tracking.run_name)\n",
        ") as parent_run:\n",
        "    \n",
        "    print(parent_run.url)  # Run URL\n",
        "    parent_run.set_tags(tags)  # Set tags\n",
        "    parent_run.log_params(params.dict())  # Log params\n",
        "\n",
        "    # In case this parent run has been resumed by ID, find out which model versions\n",
        "    # have already been evaluated by successfull child runs, so we can skip them.\n",
        "    skip_versions = set()\n",
        "    for child_run in parent_run.children:\n",
        "        if child_run.status == mlopus.mlflow.RunStatus.FINISHED:\n",
        "            skip_versions = skip_versions.union(\n",
        "                mlopus.lineage.of(child_run).inputs.models[params.model_name]\n",
        "            )\n",
        "\n",
        "    # Iterate model versions that haven't been evaluated yet\n",
        "    for model_version in params.model_versions.difference(skip_versions):\n",
        "\n",
        "        # Evaluate the model version in a child run to obtain metrics and artifacts\n",
        "        with (\n",
        "            parent_run.start_child(f\"eval_v{model_version}\") as child_run,\n",
        "            experiment_logic(model_version) as (metrics, artifacts),\n",
        "        ):\n",
        "            \n",
        "            print(child_run.url)  # Run URL\n",
        "            child_run.set_tags(tags)  # Set tags\n",
        "            child_run.log_params(params.copy(update={\"model_versions\": {model_version}}).dict())\n",
        "            \n",
        "            # Register the evaluated model version as an input of the child run\n",
        "            mlopus.lineage.of(child_run) \\\n",
        "                .with_input_model(params.model_name, model_version) \\\n",
        "                .register()\n",
        "            \n",
        "            # Iterate artifacts\n",
        "            for file_name, file_path_or_dumper in artifacts.items():\n",
        "                child_run.log_artifact(file_path_or_dumper, file_name)  # Log artifact\n",
        "            \n",
        "            child_run.log_metrics(metrics)  # Log metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1b23c3-5293-4aa2-8acf-7c7b5e51b313",
      "metadata": {},
      "source": [
        "### Experiment summary\n",
        "Aggregate metrics from child runs into the parent run and publish reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e18a332-46fa-425e-b8c9-c2e76dcd0c4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_by_vector_size = {}\n",
        "\n",
        "# Iterate child runs\n",
        "for child_run in parent_run.children:\n",
        "    if child_run.status != mlopus.mlflow.RunStatus.FINISHED:\n",
        "        continue  # Skip failed runs\n",
        "\n",
        "    # Find which model version was evaluated by this child run\n",
        "    evaluated_version = iter_utils.get_one(\n",
        "        mlopus.lineage.of(child_run).inputs.models[params.model_name]\n",
        "    )\n",
        "    \n",
        "    # Get the model version metadata\n",
        "    version_meta = mlflow.get_model(params.model_name).get_version(evaluated_version)\n",
        "\n",
        "    # Get the vector size used by the run that produced this model\n",
        "    vector_size = version_meta.run.params[\"vectors\"][\"D\"]\n",
        "\n",
        "    # Store values of average ANN distances at K for this vector size\n",
        "    metrics_by_vector_size[vector_size] = child_run.metrics[\"avg_dist_at\"]\n",
        "\n",
        "# Create dataframe of average ANN distances at K by vector size\n",
        "print(df := pd.DataFrame(metrics_by_vector_size).sort_index(axis=1).rename_axis(\"k\"))\n",
        "\n",
        "# Save dataframe as CSV file in the parent run artifacts\n",
        "parent_run.log_artifact(df.to_csv, \"avg_dist_at_k_by_vector_size.csv\")\n",
        "\n",
        "# Create a plot of mean distances at K by vector size and save to parent run artifacts\n",
        "for vector_size in df:\n",
        "    plt.plot(df.index, df[vector_size], marker='o', label=f'D={vector_size}')\n",
        "\n",
        "plt.ylabel('Mean neighbour distance')\n",
        "plt.xlabel('K-values (ANN max neighbours)')\n",
        "plt.title('Mean ANN distances at K by vector size')\n",
        "plt.legend()\n",
        "\n",
        "parent_run.log_artifact(\n",
        "    path_in_run=\"avg_dist_at_k_by_vector_size.png\",\n",
        "    source=functools.partial(plt.savefig, format=\"png\"),\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4be4ed7-495e-4290-9059-a3f10b456958",
      "metadata": {},
      "source": [
        "Here's a compact version of the loop from the previous step, using dict comprehension:\n",
        "```python\n",
        "metrics_by_vector_size = {\n",
        "    mlflow \\\n",
        "        .get_model(params.model_name) \\\n",
        "        .get_version(\n",
        "            iter_utils.get_one(\n",
        "                mlopus.lineage.of(child_run).inputs.models[params.model_name]\n",
        "            )\n",
        "        ) \\\n",
        "        .run.params[\"vectors\"][\"D\"]: child_run.metrics[\"avg_dist_at\"]\n",
        "    \n",
        "    for child_run in parent_run.children\n",
        "    if child_run.status == mlopus.mlflow.RunStatus.FINISHED\n",
        "}\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "language": "python",
      "name": ""
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
